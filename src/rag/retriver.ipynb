{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retriver and Chain with Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be using previous embedding stored in weviate vector store\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_weaviate.vectorstores import WeaviateVectorStore\n",
    "from weaviate.auth import Auth\n",
    "import os\n",
    "import weaviate\n",
    "\n",
    "client = weaviate.connect_to_weaviate_cloud(\n",
    "            cluster_url=f'https://{os.getenv(\"WEAVIATE_URL\")}',\n",
    "            auth_credentials=Auth.api_key(os.getenv('WEAVIATE_API_KEY')),\n",
    "            headers={\n",
    "                    'X-OpenAI-Api-Key': os.getenv('OPENAI_API_KEY') # openai api key for vectorizer & generative\n",
    "                }\n",
    "        )\n",
    "\n",
    "open_ai_embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_conection = WeaviateVectorStore(client=client, index_name=\"Attention\", text_key=\"content\", embedding=open_ai_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "\n",
    "# load the openAI model\n",
    "openai_llm = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desgin the prompt template\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following qestions based only on the provided context.\n",
    "think step by step before providing a detailed answer.\n",
    "I will tip you $100 if the user find the answer helpful.\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## chain introduction\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "document_chain = create_stuff_documents_chain(openai_llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['WeaviateVectorStore', 'OpenAIEmbeddings'], vectorstore=<langchain_weaviate.vectorstores.WeaviateVectorStore object at 0x000002A14B89B2F0>, search_type='mmr', search_kwargs={'k': 5, 'lambda_mult': 0.7})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "A retriever is an interface that returns documents given an unstructured query. It is more general than a vector store.\n",
    "A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores,\n",
    "but are also broad enough to include Wikipedia search and Amazon Kendra.\n",
    "\n",
    "Retrievers accept a string query as input and return a list of Documents as output.\n",
    "hybrid_search ={\n",
    "    search_type=\"hybrid\",\n",
    "    search_kwargs={\n",
    "        \"alpha\": 0.7,    # 70% vector, 30% keyword\n",
    "        \"k\": 5,\n",
    "        \"query\": \"LangChain integration with RAG\"\n",
    "    }\n",
    "\"\"\"\n",
    "\n",
    "retriver = db_conection.as_retriever(\n",
    "            search_type=\"mmr\",  # or \"similarity\"\n",
    "            search_kwargs={\"k\": 5, \"lambda_mult\": 0.7}\n",
    "        )\n",
    "retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' in the context of multi-head attention\\nAnswer: The attention function for multi-head attention can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum of the values, where the weights are determined by the compatibility between the query and each key. This compatibility can be computed using either additive attention or dot-product attention, with the latter being more efficient due to the use of a scaling factor of 1/âˆšdk.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "retriver_chain = create_retrieval_chain(retriver, document_chain)\n",
    "response = retriver_chain.invoke({'input': 'give me the function for attention mechanisim'})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
